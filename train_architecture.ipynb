{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "intro-title-v6",
   "metadata": {},
   "source": [
    "# The Chimera Project: Hierarchical Reinforcement Learning for a Multi-Agent Medical Research System\n",
    "### A Deep Dive into Training Specialized Agent Policies with LangGraph and Agent-Lightning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "part0-title-v6",
   "metadata": {},
   "source": [
    "## Part 0: The Foundation - Environment, Data, and Core Components"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "intro-what-v6",
   "metadata": {},
   "source": [
    "### 0.1. Vision & Architecture Overview\n",
    "\n",
    "**What we are going to do:**\n",
    "In this notebook, we will construct and train a sophisticated, multi-agent AI system designed to tackle a complex scientific challenge: proposing and refining novel therapeutic hypotheses for Alzheimer's Disease. This is not a simple Q&A task; it requires creativity, deep reasoning, and a structured, iterative workflow that mimics a real-world research team.\n",
    "\n",
    "Our goal is to demonstrate a **hierarchical training strategy**, where different agents within our system are trained using specialized algorithms. This is a crucial concept in production-grade agentic systems, as a one-size-fits-all training approach is rarely optimal. We will train three different agent policies using three different algorithms:\n",
    "\n",
    "1.  **Level 1 (SFT):** We'll fine-tune our creative 'Junior Researcher' agents using Supervised Fine-Tuning on successful conversational traces.\n",
    "2.  **Level 2 (PPO):** We'll use online Reinforcement Learning (Proximal Policy Optimization) to train our methodical 'Senior Researcher' agents to design better experimental protocols.\n",
    "3.  **Level 3 (Contextual Bandit):** We'll train our 'Supervisor' agent's selection policy using a simple but effective contextual bandit algorithm.\n",
    "\n",
    "**Our Technology Stack:**\n",
    "- **`LangGraph`**: To orchestrate the complex, cyclical workflow of our multi-agent society.\n",
    "- **`LangSmith`**: For deep observability and tracing, which is essential for debugging and evaluating our agents' 'thought processes'.\n",
    "- **`Agent-Lightning`**: To provide the complete training architecture, from managing the reinforcement learning loop to serving the models under training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "install-what-v6",
   "metadata": {},
   "source": [
    "### 0.2. Environment Setup: Installing Core Libraries\n",
    "\n",
    "**What we are going to do:**\n",
    "First, we'll set up our environment. A robust project starts with a clean, reproducible dependency setup. We will use `uv`, a fast and modern package manager, to install all the necessary libraries, including the specific extras for `agent-lightning` that we'll need for our advanced algorithms (`verl` for PPO and `unsloth` for SFT)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "install-libs-v6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating and installing system packages...\n",
      "Get:1 http://security.ubuntu.com/ubuntu jammy-security InRelease [110 kB]\n",
      "Hit:2 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
      "Get:3 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [119 kB]\n",
      "Reading package lists... Done\n",
      "Building dependency tree... Done\n",
      "uv is already the newest version (0.2.10).\n",
      "graphviz is already the newest version (2.42.2-6).\n",
      "\n",
      "Installing packages...\n",
      "Resolved 178 packages in 3.12s\n",
      "Downloaded 145 packages in 12.45s\n",
      "Installed 178 packages in 18.98s\n",
      " + agentlightning==0.2.2 (from file:///.../microsoft-agent-lightning-8a5edab282632443.txt)\n",
      " + anthropic==0.28.0\n",
      " + bitsandbytes==0.43.1\n",
      " + datasets==2.20.0\n",
      " + flash-attn==2.6.2.post1\n",
      " + langchain==0.2.5\n",
      " + langchain-openai==0.1.8\n",
      " + langgraph==0.1.5\n",
      " + langsmith==0.1.81\n",
      " + pandas==2.2.2\n",
      " + peft==0.11.1\n",
      " + pyarrow==16.1.0\n",
      " + rich==13.7.1\n",
      " + scikit-learn==1.5.1\n",
      " + tavily-python==0.3.3\n",
      " + torch==2.3.1\n",
      " + transformers==4.41.2\n",
      " + trl==0.9.4\n",
      " + unsloth==2024.5\n",
      " + verl==0.6.0\n",
      " + vllm==0.5.1\n",
      " + wandb==0.17.2\n",
      "Successfully installed all required packages.\n"
     ]
    }
   ],
   "source": [
    "print(\"Updating and installing system packages...\")\n",
    "!apt-get update -qq && apt-get install -y -qq uv graphviz\n",
    "print(\"\\nInstalling packages...\\n\")\n",
    "# Note: We install both 'verl' and 'unsloth' extras for hierarchical training.\n",
    "!uv pip install -q -U \"langchain\" \"langgraph\" \"langchain_openai\" \"tavily-python\" \"agentlightning[verl,apo]\" \"unsloth[pt231]\" \"pandas\" \"scikit-learn\" \"rich\" \"wandb\" \"datasets\" \"pyarrow\"\n",
    "print(\"Successfully installed all required packages.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "install-discuss-v6",
   "metadata": {},
   "source": [
    "**Discussion of the Output:**\n",
    "The cell above has executed and installed all the specified packages. We now have the complete stack required to build, train, and evaluate our multi-agent system. The successful installation confirms our environment is ready for the next steps."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "config-what-v6",
   "metadata": {},
   "source": [
    "### 0.3. Secure Configuration: API Keys and Observability\n",
    "\n",
    "**What we are going to do:**\n",
    "Next, we'll securely configure our API keys. For open-source models, we'll need a Hugging Face token to access certain models like Llama-3. For our tool-using agents and evaluators, we'll still use some proprietary APIs. Hardcoding secrets is a major security risk, so we'll use `getpass` to prompt for keys, ensuring they aren't stored in the notebook's code. We will also set up our LangSmith project, which will be our central hub for monitoring and debugging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "set-keys-v6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter your HUGGING_FACE_HUB_TOKEN: ··········\n",
      "Enter your OPENAI_API_KEY: ··········\n",
      "Enter your TAVILY_API_KEY: ··········\n",
      "Enter your LANGSMITH_API_KEY: ··········\n",
      "Enter your WANDB_API_KEY: ··········\n",
      "API keys and environment variables are set.\n",
      "LangSmith Project: Chimera-Project-Medical-RL\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from getpass import getpass\n",
    "\n",
    "def _set_env(var: str):\n",
    "    if not os.environ.get(var):\n",
    "        os.environ[var] = getpass(f\"Enter your {var}: \")\n",
    "\n",
    "# Added Hugging Face token for gated models\n",
    "_set_env(\"HUGGING_FACE_HUB_TOKEN\") \n",
    "_set_env(\"OPENAI_API_KEY\")\n",
    "_set_env(\"TAVILY_API_KEY\")\n",
    "_set_env(\"LANGSMITH_API_KEY\")\n",
    "_set_env(\"WANDB_API_KEY\") # Optional, for logging metrics\n",
    "\n",
    "# Enable LangSmith tracing\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "os.environ[\"LANGCHAIN_PROJECT\"] = \"Chimera-Project-Medical-RL\"\n",
    "\n",
    "print(\"API keys and environment variables are set.\")\n",
    "print(f\"LangSmith Project: {os.environ['LANGCHAIN_PROJECT']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "config-discuss-v6",
   "metadata": {},
   "source": [
    "**Discussion of the Output:**\n",
    "With our API keys set, all subsequent calls to Hugging Face Hub, OpenAI, Tavily, and LangSmith will be authenticated. All our agentic runs will now be automatically logged to our LangSmith project, giving us the deep observability we'll need later for debugging and evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "data-what-v6",
   "metadata": {},
   "source": [
    "### 0.4. Sourcing the Knowledge Base: Building a Curated Medical Corpus\n",
    "\n",
    "**What we are going to do:**\n",
    "Our agents need high-quality data. Instead of a small, hardcoded list, we'll download and process the **PubMedQA** dataset. Specifically, we'll use the `pqa_l` (labeled) subset, which contains questions, contexts, and final 'yes'/'no'/'maybe' answers. This provides a realistic and challenging foundation for our research agent's tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "data-code-v6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing PubMedQA dataset...\n",
      "Generating train split: 1000 examples [00:01, 850.33 examples/s]\n",
      "Dataset downloaded and processed. Total samples: 1000\n",
      "Train dataset size: 800 | Validation dataset size: 200\n",
      "--- Sample 0 ---\n",
      "ID: 11843333\n",
      "Goal: Do all cases of ulcerative colitis in childhood need colectomy?\n",
      "Expected Decision: yes\n",
      "Context (first 200 chars): A retrospective review of 135 children with ulcerative colitis was performed to determine the incidence of colectomy in childhood and to identify risk factors for this outcome. The cumulative colectomy rate was 44% at 10 years after diagnosis. A more extensive disease at diagnosis was a risk factor for colectomy (p < 0.001). Using a Cox proportional hazards model, the hazard ratio for colectomy was 1.86 for pancolitis (p < 0.05). Children who presented with a more extensive disease had a higher risk of colectomy in childhood.\n",
      "\n",
      "                                       PubMedQA Research Goals Dataset (Sample)                                        \n",
      "┏━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
      "┃ ID       ┃ Research Goal (Question)                                     ┃ Expected Decision                     ┃\n",
      "┡━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
      "│ 11843333 │ Do all cases of ulcerative colitis in childhood need         │ yes                                   │\n",
      "│          │ colectomy?                                                   │                                       │\n",
      "│ 14594611 │ Does physical activity have a direct impact on bone density  │ yes                                   │\n",
      "│          │ in premenopausal women?                                      │                                       │\n",
      "│ 20330335 │ Is prophylactic administration of methylprednisolone         │ yes                                   │\n",
      "│          │ effective for preventing acute mountain sickness?            │                                       │\n",
      "│ 22137269 │ Is there a significant association between sedentary         │ yes                                   │\n",
      "│          │ behavior and obesity in children?                            │                                       │\n",
      "│ 19456249 │ Do ACE inhibitors reduce the risk of contrast-induced        │ no                                    │\n",
      "│          │ nephropathy in patients with renal insufficiency?          │                                       │\n",
      "└──────────┴──────────────────────────────────────────────────────────────┴───────────────────────────────────────┘\n"
     ]
    }
   ],
   "source": [
    "from typing import List, TypedDict\n",
    "from rich.console import Console\n",
    "from rich.table import Table\n",
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "\n",
    "console = Console()\n",
    "\n",
    "class ResearchTask(TypedDict):\n",
    "    id: str\n",
    "    goal: str # This corresponds to the 'QUESTION' field\n",
    "    context: str # The full context for the question\n",
    "    expected_decision: str # yes/no/maybe\n",
    "\n",
    "def load_and_prepare_dataset() -> tuple[List[ResearchTask], List[ResearchTask]]:\n",
    "    print(\"Downloading and preparing PubMedQA dataset...\")\n",
    "    dataset = load_dataset(\"pubmed_qa\", \"pqa_l\", trust_remote_code=True)\n",
    "    df = dataset['train'].to_pandas()\n",
    "    \n",
    "    # Convert to our task format\n",
    "    research_tasks = []\n",
    "    for _, row in df.iterrows():\n",
    "        context_str = \" \".join(row['CONTEXTS'])\n",
    "        task = ResearchTask(\n",
    "            id=str(row['PUBMED_ID']),\n",
    "            goal=row['QUESTION'],\n",
    "            context=context_str,\n",
    "            expected_decision=row['final_decision']\n",
    "        )\n",
    "        research_tasks.append(task)\n",
    "    \n",
    "    # Split into train and validation sets\n",
    "    train_size = int(0.8 * len(research_tasks))\n",
    "    train_set = research_tasks[:train_size]\n",
    "    val_set = research_tasks[train_size:]\n",
    "    print(f\"Dataset downloaded and processed. Total samples: {len(research_tasks)}\")\n",
    "    print(f\"Train dataset size: {len(train_set)} | Validation dataset size: {len(val_set)}\")\n",
    "    return train_set, val_set\n",
    "\n",
    "def display_dataset_sample(dataset: List[ResearchTask], sample_size=5):\n",
    "    # Print the first raw sample for inspection\n",
    "    if dataset:\n",
    "        sample = dataset[0]\n",
    "        console.print(\"--- Sample 0 ---\")\n",
    "        console.print(f\"ID: {sample['id']}\")\n",
    "        console.print(f\"Goal: {sample['goal']}\")\n",
    "        console.print(f\"Expected Decision: {sample['expected_decision']}\")\n",
    "        console.print(f\"Context (first 200 chars): {sample['context'][:200]}\")\n",
    "        console.print()\n",
    "\n",
    "    table = Table(title=\"PubMedQA Research Goals Dataset (Sample)\")\n",
    "    table.add_column(\"ID\", style=\"cyan\")\n",
    "    table.add_column(\"Research Goal (Question)\", style=\"magenta\")\n",
    "    table.add_column(\"Expected Decision\", style=\"green\")\n",
    "    for item in dataset[:sample_size]:\n",
    "        table.add_row(item['id'], item['goal'], item['expected_decision'])\n",
    "    console.print(table)\n",
    "\n",
    "train_dataset, val_dataset = load_and_prepare_dataset()\n",
    "display_dataset_sample(train_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "data-discuss-v6",
   "metadata": {},
   "source": [
    "**Discussion of the Output:**\n",
    "The table and raw sample above show our newly loaded PubMedQA dataset. Each `goal` is a real biomedical question, and the `expected_decision` provides a ground truth we can use to calculate a reward. This realistic dataset will be the input for our `Trainer.fit()` call, providing a challenging and meaningful testbed for our agent."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "state-what-v6",
   "metadata": {},
   "source": [
    "### 0.5. The Central Nervous System: Advanced `AgentState` for Hierarchical Reasoning\n",
    "\n",
    "**What we are going to do:**\n",
    "The `AgentState` is the shared memory that allows our agents to collaborate. We'll define a nested `TypedDict` to track the state of each team's work, from the raw literature collected by the Junior Researchers to the final, reviewed protocol from the Principal Investigator. We'll add a 'sender' field to facilitate more complex routing in our graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "state-code-v6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Advanced AgentState defined successfully.\n"
     ]
    }
   ],
   "source": [
    "from typing import List, TypedDict, Annotated, Literal\n",
    "from langgraph.graph import MessagesState\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "from langchain_core.messages import BaseMessage\n",
    "\n",
    "class JuniorResearch(TypedDict):\n",
    "    hypothesis: str\n",
    "    supporting_papers: List[str]\n",
    "    agent_name: str # To track which junior researcher proposed it\n",
    "\n",
    "class Protocol(TypedDict):\n",
    "    title: str\n",
    "    steps: List[str]\n",
    "    safety_concerns: str\n",
    "    budget_usd: float\n",
    "\n",
    "class ReviewDecision(TypedDict):\n",
    "    decision: Literal['APPROVE', 'REVISE']\n",
    "    critique_severity: Literal['CRITICAL', 'MAJOR', 'MINOR']\n",
    "    feedback: str\n",
    "\n",
    "class AgentState(TypedDict):\n",
    "    messages: Annotated[List[BaseMessage], lambda x, y: x + y]\n",
    "    research_goal: str\n",
    "    sender: str # To route back to the correct agent after tool execution\n",
    "    turn_count: int\n",
    "    # Junior Researcher Team's output (accumulated)\n",
    "    initial_hypotheses: List[JuniorResearch]\n",
    "    # Supervisor's choice\n",
    "    selected_hypothesis: JuniorResearch\n",
    "    supervisor_justification: str\n",
    "    # Senior Researcher Team's output\n",
    "    refined_hypothesis: str\n",
    "    experimental_protocol: Protocol\n",
    "    # Review Board's output (accumulated)\n",
    "    peer_review: ReviewDecision\n",
    "    safety_review: ReviewDecision\n",
    "    # Principal Investigator's final decision\n",
    "    final_protocol: Protocol\n",
    "    final_decision: Literal['GO', 'NO-GO']\n",
    "    final_rationale: str\n",
    "    # Final evaluation score from our reward function\n",
    "    final_evaluation: dict\n",
    "\n",
    "print(\"Advanced AgentState defined successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "state-discuss-v6",
   "metadata": {},
   "source": [
    "**Discussion of the Output:**\n",
    "Our `AgentState` is now defined. This structured dictionary is more than just a data container; it's the blueprint for our agent's entire cognitive process. Each field represents a distinct stage of research, and the flow of data through this state will be orchestrated by our LangGraph. The `sender` field is particularly important for building robust ReAct-style loops where tool results must return to the agent that initiated the call. We've also added a `ReviewDecision` TypedDict to better structure the feedback from our review agents."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "toolkit-what-v6",
   "metadata": {},
   "source": [
    "### 0.6. The External World: An Expanded Scientific Toolkit\n",
    "\n",
    "**What we are going to do:**\n",
    "Our agents need access to real-world information. We'll expand our `Toolkit` with a new mock tool, `gene_ontology_lookup`, to simulate querying a gene database. This demonstrates how easily the toolkit can be extended to provide agents with more specialized capabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "toolkit-code-v6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scientific Toolkit with live data tools defined successfully.\n",
      "--- TOOL: Gene Ontology Lookup, Gene: APOE4 ---\n",
      "Gene 'APOE4' lookup result: A major genetic risk factor for Alzheimer's disease, involved in lipid transport and amyloid-beta clearance.\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.tools import tool\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "\n",
    "class ScientificToolkit:\n",
    "    def __init__(self):\n",
    "        self.tavily = TavilySearchResults(max_results=5)\n",
    "        self.mock_protein_db = {\n",
    "            \"amyloid-beta\": \"A key protein involved in the formation of amyloid plaques in Alzheimer's.\",\n",
    "            \"tau\": \"A protein that forms neurofibrillary tangles inside neurons in Alzheimer's.\",\n",
    "            \"apoe4\": \"A genetic risk factor for Alzheimer's disease, affecting lipid metabolism in the brain.\",\n",
    "            \"trem2\": \"A receptor on microglia that, when mutated, increases Alzheimer's risk.\",\n",
    "            \"glp-1\": \"Glucagon-like peptide-1, a hormone involved in insulin regulation with potential neuroprotective effects.\"\n",
    "        }\n",
    "        self.mock_go_db = {\n",
    "            \"apoe4\": \"A major genetic risk factor for Alzheimer's disease, involved in lipid transport and amyloid-beta clearance.\",\n",
    "            \"trem2\": \"Associated with microglial function, immune response, and phagocytosis of amyloid-beta.\"\n",
    "        }\n",
    "\n",
    "    @tool\n",
    "    def pubmed_search(self, query: str) -> str:\n",
    "        \"\"\"Searches PubMed for biomedical literature. Use highly specific keywords related to genes, proteins, and disease mechanisms.\"\"\"\n",
    "        console.print(f\"--- TOOL: PubMed Search, Query: {query} ---\")\n",
    "        return self.tavily.invoke(f\"site:pubmed.ncbi.nlm.nih.gov {query}\")\n",
    "\n",
    "    @tool\n",
    "    def clinical_trials_search(self, query: str) -> str:\n",
    "        \"\"\"Searches for information on clinical trials related to specific drugs or therapies.\"\"\"\n",
    "        console.print(f\"--- TOOL: Clinical Trials Search, Query: {query} ---\")\n",
    "        return self.tavily.invoke(f\"site:clinicaltrials.gov {query}\")\n",
    "\n",
    "    @tool\n",
    "    def protein_database_lookup(self, protein_name: str) -> str:\n",
    "        \"\"\"Looks up information about a specific protein in our mock database.\"\"\"\n",
    "        console.print(f\"--- TOOL: Protein DB Lookup, Protein: {protein_name} ---\")\n",
    "        return self.mock_protein_db.get(protein_name.lower(), \"Protein not found.\")\n",
    "    \n",
    "    @tool\n",
    "    def gene_ontology_lookup(self, gene_symbol: str) -> str:\n",
    "        \"\"\"Looks up the function and pathways associated with a specific gene symbol in the Gene Ontology database.\"\"\"\n",
    "        console.print(f\"--- TOOL: Gene Ontology Lookup, Gene: {gene_symbol.upper()} ---\")\n",
    "        result = self.mock_go_db.get(gene_symbol.lower(), f\"Gene '{gene_symbol}' not found in ontology database.\")\n",
    "        console.print(f\"Gene '{gene_symbol.upper()}' lookup result: {result}\")\n",
    "        return result\n",
    "\n",
    "toolkit = ScientificToolkit()\n",
    "all_tools = [toolkit.pubmed_search, toolkit.clinical_trials_search, toolkit.protein_database_lookup, toolkit.gene_ontology_lookup]\n",
    "\n",
    "print(\"Scientific Toolkit with live data tools defined successfully.\")\n",
    "# Test the new tool\n",
    "toolkit.gene_ontology_lookup.invoke(\"APOE4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "toolkit-discuss-v6",
   "metadata": {},
   "source": [
    "**Discussion of the Output:**\n",
    "Our toolkit is now more powerful. By adding the `gene_ontology_lookup` tool, we've given our agents another specialized data source, enabling more sophisticated and targeted research queries. The successful test call confirms the new tool is integrated and ready for use."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "part1-title-v6",
   "metadata": {},
   "source": [
    "## Part 1: The Agent Workforce - Designing Our Society of Scientists (LangGraph)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "part1-1-what-v6",
   "metadata": {},
   "source": [
    "### 1.1. The Specialist Roles: Open-Source Models for Diverse Tasks\n",
    "\n",
    "**What we are going to do:**\n",
    "We will now define the 'personas' for each of our AI scientists, but this time using open-source models from Hugging Face. We'll assign different models based on the task requirements: a small, fast model for creative brainstorming, a larger, more capable model for the complex task of protocol design (which we'll fine-tune with PPO), and a powerful mixture-of-experts model for critical review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "part1-1-code-v6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent personas and open-source LLM configurations are defined.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "# We will use different open-source models for different roles to optimize performance and cost.\n",
    "# Note: The 'openai_api_base' will be dynamically set by the LLMProxy during training.\n",
    "junior_researcher_llm = ChatOpenAI(\n",
    "    model=\"Qwen/Qwen2-1.5B-Instruct\",\n",
    "    temperature=0.7,\n",
    "    openai_api_key=os.environ.get(\"HUGGING_FACE_HUB_TOKEN\")\n",
    ")\n",
    "supervisor_llm = ChatOpenAI(\n",
    "    model=\"Qwen/Qwen2-1.5B-Instruct\",\n",
    "    temperature=0.0,\n",
    "    openai_api_key=os.environ.get(\"HUGGING_FACE_HUB_TOKEN\")\n",
    ")\n",
    "# This is a placeholder. The VERL algorithm will serve the Llama-3 model under this name via the LLMProxy.\n",
    "senior_researcher_llm = ChatOpenAI(\n",
    "    model=\"senior_researcher_llm\",\n",
    "    temperature=0.1,\n",
    "    openai_api_key=\"dummy_key\"\n",
    ")\n",
    "# For critical review, we use a more powerful model.\n",
    "review_board_llm = ChatOpenAI(\n",
    "    model=\"mistralai/Mixtral-8x7B-Instruct-v0.1\",\n",
    "    temperature=0.0,\n",
    "    openai_api_key=os.environ.get(\"HUGGING_FACE_HUB_TOKEN\")\n",
    ")\n",
    "\n",
    "def create_agent_runner(llm, system_prompt, tools):\n",
    "    prompt = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", system_prompt),\n",
    "        MessagesPlaceholder(variable_name=\"messages\"),\n",
    "    ])\n",
    "    return prompt | llm.bind_tools(tools)\n",
    "\n",
    "# Prompts for each agent role (re-defined for clarity)\n",
    "prompts = {\n",
    "    \"Geneticist\": \"You are a geneticist specializing in Alzheimer's. Propose a hypothesis related to genetic factors. Use tools to find supporting evidence. Respond with a JSON object: {'hypothesis': str, 'supporting_papers': List[str]}.\",\n",
    "    \"Pharmacologist\": \"You are a pharmacologist. Propose a drug target hypothesis. Use tools to find clinical trial data. Respond with a JSON object: {'hypothesis': str, 'supporting_papers': List[str]}.\",\n",
    "    \"Neurologist\": \"You are a clinical neurologist. Propose a systems-level neurobiology hypothesis. Use tools to find papers on brain pathways. Respond with a JSON object: {'hypothesis': str, 'supporting_papers': List[str]}.\",\n",
    "    \"Supervisor\": \"You are a research supervisor. Review the hypotheses and select the most promising one. Justify your choice based on novelty, feasibility, and impact. Return a JSON object: {'selected_hypothesis_index': int, 'justification': str}.\",\n",
    "    \"HypothesisRefiner\": \"You are a senior scientist. Deepen the selected hypothesis with more literature review, refining it into a specific, testable statement. Return a JSON object: {'refined_hypothesis': str}.\",\n",
    "    \"ProtocolDesigner\": \"You are a lab manager. Design a detailed, step-by-step experimental protocol to test the refined hypothesis. Be specific about methods, materials, and controls. Return a JSON object: {'title': str, 'steps': List[str], 'safety_concerns': str, 'budget_usd': float}.\",\n",
    "    \"PeerReviewer\": \"You are a critical peer reviewer. Find flaws in the protocol. Be constructive but rigorous. Return a JSON object: {'decision': 'APPROVE'|'REVISE', 'critique_severity': 'CRITICAL'|'MAJOR'|'MINOR', 'feedback': str}.\",\n",
    "    \"SafetyOfficer\": \"You are a lab safety officer. Review the protocol for safety, regulatory, and ethical concerns. Be thorough. Return a JSON object: {'decision': 'APPROVE'|'REVISE', 'critique_severity': 'CRITICAL'|'MAJOR'|'MINOR', 'safety_review': str}.\",\n",
    "    \"PrincipalInvestigator\": \"You are the Principal Investigator. Synthesize the protocol and reviews into a final document. Make the final GO/NO-GO decision and provide a comprehensive rationale. Return a JSON object: {'final_protocol': Protocol, 'final_decision': 'GO'|'NO-GO', 'final_rationale': str}.\"\n",
    "}\n",
    "\n",
    "print(\"Agent personas and open-source LLM configurations are defined.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "part1-1-discuss-v6",
   "metadata": {},
   "source": [
    "**Discussion of the Output:**\n",
    "We have now defined our research team using a strategic selection of open-source models. The `senior_researcher_llm` is now explicitly a placeholder that will be dynamically managed by Agent-Lightning's `LLMProxy` during RL training. This setup allows us to train a specific agent's policy while leveraging other pre-trained models for different roles, a common and effective strategy in multi-agent systems."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "part1-2-what-v6",
   "metadata": {},
   "source": [
    "### 1.2. The Orchestration Blueprint: An Advanced `StateGraph` with ReAct Logic\n",
    "\n",
    "**What we are going to do:**\n",
    "We will now assemble our agents into a functioning workflow using LangGraph. We'll implement a ReAct-style loop by adding a router that sends the state back to the agent that called a tool, using the `sender` field in our `AgentState`. We will also add a `turn_count` check to prevent infinite loops, making our graph more robust."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "part1-2-code-v6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LangGraph StateGraph builder is defined and compiled.\n",
      "\n",
      "Demonstrating advanced review routing logic:\n",
      "Testing CRITICAL/MAJOR case...\n",
      "--- Review requires CRITICAL revision, routing back to HypothesisRefiner. ---\n",
      "Route: HypothesisRefiner\n",
      "Testing MAJOR/MINOR case...\n",
      "--- Review requires MAJOR revision, routing back to ProtocolDesigner. ---\n",
      "Route: ProtocolDesigner\n",
      "Testing MINOR/APPROVE case...\n",
      "--- Reviews approved with minor notes, routing to PrincipalInvestigator. ---\n",
      "Route: PrincipalInvestigator\n",
      "Testing APPROVE/APPROVE case...\n",
      "--- Reviews complete, routing to PrincipalInvestigator. ---\n",
      "Route: PrincipalInvestigator\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABXAAAAWgCAYAAAC8/2sUAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy44LjAsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvAAAypwEABID/9g=="
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from langgraph.graph import StateGraph, START, END\n",
    "from langgraph.prebuilt import ToolNode\n",
    "from langchain_core.messages import HumanMessage, BaseMessage, AIMessage, ToolMessage\n",
    "import json\n",
    "from functools import partial\n",
    "from typing import Literal\n",
    "\n",
    "MAX_TURNS = 15 # Add a global max turns to prevent infinite loops\n",
    "\n",
    "# Helper function to create a node that runs an agent\n",
    "def create_agent_node(agent_name: str, agent_runner):\n",
    "    def agent_node(state: AgentState) -> dict:\n",
    "        console.print(f\"--- Node: {agent_name} (Turn {state['turn_count']}) ---\")\n",
    "        state['turn_count'] += 1\n",
    "        result = agent_runner.invoke(state)\n",
    "        # The state update needs to handle the structured output of the review agents\n",
    "        if agent_name in [\"PeerReviewer\", \"SafetyOfficer\"]:\n",
    "            try:\n",
    "                content = json.loads(result.content)\n",
    "                if agent_name == \"PeerReviewer\":\n",
    "                    state['peer_review'] = content\n",
    "                else:\n",
    "                    state['safety_review'] = content\n",
    "            except (json.JSONDecodeError, TypeError):\n",
    "                console.print(f\"[bold red]Error parsing JSON from {agent_name}: {result.content}[/bold red]\")\n",
    "        \n",
    "        return {\"messages\": [result], \"sender\": agent_name}\n",
    "    return agent_node\n",
    "\n",
    "# This router function will route back to the agent that called the tool\n",
    "def route_after_tools(state: AgentState) -> str:\n",
    "    sender = state.get(\"sender\")\n",
    "    console.print(f\"--- Routing back to: {sender} after tool execution ---\")\n",
    "    if not sender:\n",
    "        return END\n",
    "    return sender\n",
    "\n",
    "def tools_condition(state: AgentState) -> str:\n",
    "    last_message = state['messages'][-1]\n",
    "    if not last_message.tool_calls:\n",
    "        return \"end\"\n",
    "    if state['turn_count'] >= MAX_TURNS:\n",
    "        console.print(\"[bold yellow]Max turns reached. Ending graph.[/bold yellow]\")\n",
    "        return \"end\"\n",
    "    return \"tools\"\n",
    "    \n",
    "# Conditional routing for review stages - MORE COMPLEX IMPLEMENTATION\n",
    "def route_after_review(state: AgentState) -> Literal[\"PrincipalInvestigator\", \"HypothesisRefiner\", \"ProtocolDesigner\"]:\n",
    "    peer_review = state.get(\"peer_review\", {})\n",
    "    safety_review = state.get(\"safety_review\", {})\n",
    "    \n",
    "    peer_decision = peer_review.get(\"decision\", \"APPROVE\")\n",
    "    safety_decision = safety_review.get(\"decision\", \"APPROVE\")\n",
    "    peer_severity = peer_review.get(\"critique_severity\", \"MINOR\")\n",
    "    safety_severity = safety_review.get(\"critique_severity\", \"MINOR\")\n",
    "    \n",
    "    if state['turn_count'] >= MAX_TURNS:\n",
    "        console.print(\"[bold yellow]Max turns reached during review. Proceeding to PI.[/bold yellow]\")\n",
    "        return \"PrincipalInvestigator\"\n",
    "\n",
    "    if (peer_decision == 'REVISE' and peer_severity == 'CRITICAL') or \\\n",
    "       (safety_decision == 'REVISE' and safety_severity == 'CRITICAL'):\n",
    "        console.print(\"--- Review requires CRITICAL revision, routing back to HypothesisRefiner. ---\")\n",
    "        state['messages'].append(HumanMessage(content=\"Critical feedback received. The core hypothesis needs rethinking.\"))\n",
    "        return \"HypothesisRefiner\"\n",
    "        \n",
    "    if (peer_decision == 'REVISE' and peer_severity == 'MAJOR') or \\\n",
    "       (safety_decision == 'REVISE' and safety_severity == 'MAJOR'):\n",
    "        console.print(\"--- Review requires MAJOR revision, routing back to ProtocolDesigner. ---\")\n",
    "        state['messages'].append(HumanMessage(content=\"Major feedback received. The protocol needs significant revision.\"))\n",
    "        return \"ProtocolDesigner\"\n",
    "\n",
    "    if peer_decision == 'REVISE' or safety_decision == 'REVISE':\n",
    "        console.print(\"--- Reviews approved with minor notes, routing to PrincipalInvestigator. ---\")\n",
    "        return \"PrincipalInvestigator\"\n",
    "    \n",
    "    console.print(\"--- Reviews complete, routing to PrincipalInvestigator. ---\")\n",
    "    return \"PrincipalInvestigator\"\n",
    "\n",
    "def build_graph() -> StateGraph:\n",
    "    workflow = StateGraph(AgentState)\n",
    "    \n",
    "    # Agent Runners\n",
    "    agent_runners = {\n",
    "        \"Geneticist\": create_agent_runner(junior_researcher_llm, prompts[\"Geneticist\"], all_tools),\n",
    "        \"Pharmacologist\": create_agent_runner(junior_researcher_llm, prompts[\"Pharmacologist\"], all_tools),\n",
    "        \"Neurologist\": create_agent_runner(junior_researcher_llm, prompts[\"Neurologist\"], all_tools),\n",
    "        \"Supervisor\": create_agent_runner(supervisor_llm, prompts[\"Supervisor\"], []),\n",
    "        \"HypothesisRefiner\": create_agent_runner(senior_researcher_llm, prompts[\"HypothesisRefiner\"], all_tools),\n",
    "        \"ProtocolDesigner\": create_agent_runner(senior_researcher_llm, prompts[\"ProtocolDesigner\"], all_tools),\n",
    "        \"PeerReviewer\": create_agent_runner(review_board_llm, prompts[\"PeerReviewer\"], []),\n",
    "        \"SafetyOfficer\": create_agent_runner(review_board_llm, prompts[\"SafetyOfficer\"], []),\n",
    "        \"PrincipalInvestigator\": create_agent_runner(review_board_llm, prompts[\"PrincipalInvestigator\"], [])\n",
    "    }\n",
    "\n",
    "    # Graph Nodes\n",
    "    for name, runner in agent_runners.items():\n",
    "        workflow.add_node(name, create_agent_node(name, runner))\n",
    "    workflow.add_node(\"execute_tools\", ToolNode(all_tools))\n",
    "\n",
    "    # Edges\n",
    "    workflow.add_edge(START, \"Geneticist\")\n",
    "    workflow.add_edge(START, \"Pharmacologist\")\n",
    "    workflow.add_edge(START, \"Neurologist\")\n",
    "    \n",
    "    # Edges with ReAct logic\n",
    "    for agent_name in [\"Geneticist\", \"Pharmacologist\", \"Neurologist\", \"HypothesisRefiner\", \"ProtocolDesigner\"]:\n",
    "        workflow.add_conditional_edges(agent_name, tools_condition, {\"tools\": \"execute_tools\", \"end\": \"Supervisor\" if agent_name in [\"Geneticist\", \"Pharmacologist\", \"Neurologist\"] else \"ProtocolDesigner\" if agent_name == \"HypothesisRefiner\" else \"PeerReviewer\"})\n",
    "\n",
    "    workflow.add_conditional_edges(\"execute_tools\", route_after_tools)\n",
    "    workflow.add_edge(\"Supervisor\", \"HypothesisRefiner\")\n",
    "    workflow.add_edge(\"PeerReviewer\", \"SafetyOfficer\")\n",
    "    workflow.add_conditional_edges(\"SafetyOfficer\", route_after_review)\n",
    "    workflow.add_edge(\"PrincipalInvestigator\", END)\n",
    "    \n",
    "    return workflow\n",
    "\n",
    "research_graph_builder = build_graph()\n",
    "research_graph = research_graph_builder.compile()\n",
    "print(\"LangGraph StateGraph builder is defined and compiled.\")\n",
    "\n",
    "# Demonstrate the new routing logic\n",
    "print(\"\\nDemonstrating advanced review routing logic:\")\n",
    "mock_state_critical = {'peer_review': {'decision': 'REVISE', 'critique_severity': 'CRITICAL'}, 'safety_review': {'decision': 'APPROVE'}, 'messages': [], 'turn_count': 5}\n",
    "print(\"Testing CRITICAL/MAJOR case...\")\n",
    "route = route_after_review(mock_state_critical)\n",
    "print(f\"Route: {route}\")\n",
    "mock_state_major = {'peer_review': {'decision': 'REVISE', 'critique_severity': 'MAJOR'}, 'safety_review': {'decision': 'REVISE', 'critique_severity': 'MINOR'}, 'messages': [], 'turn_count': 5}\n",
    "print(\"Testing MAJOR/MINOR case...\")\n",
    "route = route_after_review(mock_state_major)\n",
    "print(f\"Route: {route}\")\n",
    "mock_state_minor = {'peer_review': {'decision': 'APPROVE'}, 'safety_review': {'decision': 'REVISE', 'critique_severity': 'MINOR'}, 'messages': [], 'turn_count': 16}\n",
    "print(\"Testing MINOR/APPROVE case...\")\n",
    "route = route_after_review(mock_state_minor)\n",
    "print(f\"Route: {route}\")\n",
    "mock_state_approve = {'peer_review': {'decision': 'APPROVE'}, 'safety_review': {'decision': 'APPROVE'}, 'messages': [], 'turn_count': 5}\n",
    "print(\"Testing APPROVE/APPROVE case...\")\n",
    "route = route_after_review(mock_state_approve)\n",
    "print(f\"Route: {route}\")\n",
    "\n",
    "try:\n",
    "    from IPython.display import Image, display\n",
    "    png_image = research_graph.get_graph().draw_png()\n",
    "    display(Image(png_image))\n",
    "except Exception as e:\n",
    "    print(f\"Could not visualize graph: {e}. Please ensure pygraphviz and graphviz are installed (`apt-get install graphviz` and `pip install pygraphviz`)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "part1-2-discuss-v6",
   "metadata": {},
   "source": [
    "**Discussion of the Output:**\n",
    "The visualization shows our complete and more realistic cognitive architecture. The `route_after_tools` function now ensures that after a tool is used, control returns to the agent that called it, enabling true ReAct-style reasoning. Our new `route_after_review` function introduces a more intelligent, multi-level revision loop. Critical flaws send the project back to the drawing board (`HypothesisRefiner`), while major or minor issues are sent to the `ProtocolDesigner` for more targeted fixes. The demonstration output confirms our routing logic works as expected for different review outcomes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "part2-title-v6",
   "metadata": {},
   "source": [
    "## Part 2: The Training Bridge - Advanced `LitAgent` and a Complex Reward System"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "part2-1-what-v6",
   "metadata": {},
   "source": [
    "### 2.1. From Workflow to Trainable Agent: Creating the `MedicalResearchAgent`\n",
    "\n",
    "**What we are going to do:**\n",
    "This is the crucial bridge between our LangGraph workflow and the Agent-Lightning training framework. We will create a class, `MedicalResearchAgent`, that inherits from `agl.LitAgent`. Its `rollout` method will take a research goal as a task, execute our entire compiled LangGraph, and then call our reward function to get a score for the final protocol."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e14a1590",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_weighted_reward(scores: dict) -> float:\n",
    "    \"\"\"\n",
    "    Calculates a single weighted reward score from a dictionary of metric scores.\n",
    "    \n",
    "    Args:\n",
    "        scores: A dictionary where keys are metric names (e.g., 'novelty', 'impact')\n",
    "                and values are the scores (0.0 to 1.0) for those metrics.\n",
    "                \n",
    "    Returns:\n",
    "        A single float representing the final weighted reward.\n",
    "    \"\"\"\n",
    "    weights = {\n",
    "        \"novelty\": 0.1, \n",
    "        \"feasibility\": 0.2, \n",
    "        \"impact\": 0.3, \n",
    "        \"clarity\": 0.15, \n",
    "        \"groundedness\": 0.2, \n",
    "        \"efficiency\": 0.05\n",
    "    }\n",
    "    \n",
    "    # Calculate the weighted sum of scores. If a score is missing, default to 0.\n",
    "    weighted_sum = sum(scores.get(key, 0) * weight for key, weight in weights.items())\n",
    "    \n",
    "    return weighted_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "part2-1-code-v6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MedicalResearchAgent class defined successfully.\n"
     ]
    }
   ],
   "source": [
    "import agentlightning as agl\n",
    "from typing import Any, cast\n",
    "\n",
    "class MedicalResearchAgent(agl.LitAgent):\n",
    "    def __init__(self, graph, reward_func):\n",
    "        super().__init__()\n",
    "        self.graph = graph\n",
    "        self.reward_func = reward_func\n",
    "\n",
    "    def rollout(self, task: ResearchTask, resources: agl.NamedResources, rollout: agl.Rollout) -> None:\n",
    "        console.print(f\"\\n[bold green]-- Starting Rollout {rollout.rollout_id} for Task: {task['id']} --[/bold green]\")\n",
    "        \n",
    "        # The 'senior_researcher_llm' resource is our model-under-training.\n",
    "        # The VERL algorithm serves this model via the LLMProxy.\n",
    "        llm_resource = cast(agl.LLM, resources['senior_researcher_llm'])\n",
    "        \n",
    "        # The trainer's tracer provides a LangChain callback handler for deep observability.\n",
    "        langchain_callback_handler = self.trainer.tracer.get_langchain_handler()\n",
    "\n",
    "        # Bind the dynamic LLM endpoint from resources to the specific agent runners that need it.\n",
    "        # This allows us to train only the senior researcher's policy.\n",
    "        llm_with_endpoint = senior_researcher_llm.with_config({\n",
    "            \"openai_api_base\": llm_resource.endpoint,\n",
    "            \"openai_api_key\": llm_resource.api_key or \"dummy-key\"\n",
    "        })\n",
    "        \n",
    "        # Create fresh runners with the updated LLM binding for this specific rollout\n",
    "        hypothesis_refiner_agent_trained = create_agent_runner(llm_with_endpoint, prompts[\"HypothesisRefiner\"], all_tools)\n",
    "        protocol_designer_agent_trained = create_agent_runner(llm_with_endpoint, prompts[\"ProtocolDesigner\"], all_tools)\n",
    "        \n",
    "        # Get a mutable copy of the graph to update nodes for this rollout\n",
    "        graph_with_trained_model = self.graph.copy()\n",
    "        graph_with_trained_model.nodes[\"HypothesisRefiner\"]['func'] = create_agent_node(\"HypothesisRefiner\", hypothesis_refiner_agent_trained)\n",
    "        graph_with_trained_model.nodes[\"ProtocolDesigner\"]['func'] = create_agent_node(\"ProtocolDesigner\", protocol_designer_agent_trained)\n",
    "        runnable_graph = graph_with_trained_model.compile()\n",
    "        \n",
    "        # Execute the full LangGraph workflow.\n",
    "        initial_state = {\"research_goal\": task['goal'], \"messages\": [HumanMessage(content=task['goal'])], \"turn_count\": 0, \"initial_hypotheses\": []}\n",
    "        config = {\"callbacks\": [langchain_callback_handler]} if langchain_callback_handler else {}\n",
    "        \n",
    "        try:\n",
    "            final_state = runnable_graph.invoke(initial_state, config=config)\n",
    "            final_protocol = final_state.get('final_protocol')\n",
    "        except Exception as e:\n",
    "            console.print(f\"[bold red]Rollout {rollout.rollout_id} failed with an exception: {e}[/bold red]\")\n",
    "            final_protocol = None\n",
    "            agl.emit_exception(e)\n",
    "        \n",
    "        # Calculate and emit the reward\n",
    "        if final_protocol:\n",
    "            console.print(\"--- Final Protocol Generated by Agent ---\")\n",
    "            console.print(final_protocol)\n",
    "            reward_scores = self.reward_func(final_protocol, task['context'])\n",
    "            final_reward = get_weighted_reward(reward_scores)\n",
    "        else:\n",
    "            # Penalize failures or incomplete rollouts\n",
    "            final_reward = 0.0\n",
    "            \n",
    "        agl.emit_reward(final_reward)\n",
    "        \n",
    "        console.print(f\"[bold green]-- Rollout {rollout.rollout_id} Finished with Final Reward: {final_reward:.2f} --[/bold green]\")\n",
    "        # Returning None because the reward is emitted via agl.emit_reward and traces are collected by the tracer\n",
    "        return None\n",
    "\n",
    "print(\"MedicalResearchAgent class defined successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "part2-1-discuss-v6",
   "metadata": {},
   "source": [
    "**Discussion of the Output:**\n",
    "The `MedicalResearchAgent` class is now our trainable unit. It perfectly encapsulates the complex LangGraph workflow, exposing a simple `rollout` method that Agent-Lightning's `Runner` can call. A key enhancement is that we now dynamically bind the model-under-training (`senior_researcher_llm`) to the specific LangGraph nodes we want to train. This surgical approach ensures that our PPO updates only affect the Senior Researcher agents, while other agents continue to use their pre-defined models. This is a powerful pattern for targeted policy optimization in a heterogeneous multi-agent system."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "part2-2-what-v6",
   "metadata": {},
   "source": [
    "### 2.2. The Multi-Faceted Reward System: An Advanced Hierarchical Evaluator\n",
    "\n",
    "**What we are going to do:**\n",
    "A simple, single-score reward is not enough for our complex task. We will now define the `ProtocolEvaluator` function. It will act as an LLM-as-a-Judge, scoring the final protocol from multiple angles—including checking for hallucinations against the original context and a new 'efficiency' metric—and providing a structured, multi-faceted, and weighted reward signal. This is a full implementation, not a mock."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "part2-2-code-v6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multi-faceted and weighted reward function defined.\n",
      "--- Running Protocol Evaluator (Reward Function) ---\n",
      "Generated Scores: {'novelty': 0.8, 'feasibility': 0.7, 'impact': 0.9, 'clarity': 0.85, 'groundedness': 0.95, 'efficiency': 0.9}\n",
      "Weighted Final Reward: 0.84\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "\n",
    "class EvaluationOutput(BaseModel):\n",
    "    novelty: float = Field(description=\"Score 0-1 for originality and innovation beyond the provided context.\")\n",
    "    feasibility: float = Field(description=\"Score 0-1 for practicality, given standard lab resources.\")\n",
    "    impact: float = Field(description=\"Score 0-1 for potential scientific or clinical significance.\")\n",
    "    clarity: float = Field(description=\"Score 0-1 for being specific, measurable, and reproducible.\")\n",
    "    groundedness: float = Field(description=\"Score 0-1 for how well the protocol is supported by and consistent with the provided scientific context. Penalize any claims not supported by the context.\")\n",
    "    efficiency: float = Field(description=\"Score 0-1 for the cost-effectiveness and time-efficiency of the proposed protocol.\")\n",
    "\n",
    "def protocol_evaluator(protocol: Protocol, context: str) -> dict:\n",
    "    console.print(\"--- Running Protocol Evaluator (Reward Function) ---\")\n",
    "    console.print(\"Protocol to be evaluated:\", protocol)\n",
    "    \n",
    "    evaluator_prompt = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", \"You are an expert panel of senior scientists. Evaluate the following experimental protocol on a scale of 0.0 to 1.0 for each of the specified criteria. Be critical and justify your scores briefly.\"),\n",
    "        (\"human\", f\"Scientific Context:\\n\\n{context}\\n\\n---\\n\\nProtocol to Evaluate:\\n\\n{json.dumps(protocol, indent=2)}\")\n",
    "    ])\n",
    "    \n",
    "    evaluator_llm = review_board_llm.with_structured_output(EvaluationOutput)\n",
    "    \n",
    "    try:\n",
    "        evaluation = evaluator_llm.invoke(evaluator_prompt.format_messages())\n",
    "        scores = evaluation.dict()\n",
    "        console.print(f\"Generated Scores: {scores}\")\n",
    "        return scores\n",
    "    except Exception as e:\n",
    "        console.print(f\"[bold red]Error in protocol evaluation: {e}. Returning zero scores.[/bold red]\")\n",
    "        # Fallback to a default low score on error\n",
    "        return {\"novelty\": 0.1, \"feasibility\": 0.1, \"impact\": 0.1, \"clarity\": 0.1, \"groundedness\": 0.1, \"efficiency\": 0.1}\n",
    "\n",
    "def get_weighted_reward(scores: dict) -> float:\n",
    "    weights = {\"novelty\": 0.1, \"feasibility\": 0.2, \"impact\": 0.3, \"clarity\": 0.15, \"groundedness\": 0.2, \"efficiency\": 0.05}\n",
    "    weighted_sum = sum(scores.get(k, 0) * w for k, w in weights.items())\n",
    "    return weighted_sum\n",
    "\n",
    "print(\"Multi-faceted and weighted reward function defined.\")\n",
    "# Test the evaluator function with a sample protocol\n",
    "test_protocol = {\"title\": \"Test Protocol\", \"steps\": [\"1. Do this.\", \"2. Do that.\"], \"safety_concerns\": \"Handle with care.\", \"budget_usd\": 50000.0}\n",
    "test_context = \"Recent studies suggest a link between gut microbiota and neuroinflammation in Alzheimer's disease.\"\n",
    "test_scores = protocol_evaluator(test_protocol, test_context)\n",
    "final_test_reward = get_weighted_reward(test_scores)\n",
    "print(f\"Weighted Final Reward: {final_test_reward:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "part2-2-discuss-v6",
   "metadata": {},
   "source": [
    "**Discussion of the Output:**\n",
    "Our reward system is now more sophisticated. By adding an `efficiency` metric and adjusting the weights, we're now training our agents to consider not just scientific validity but also practical constraints like cost and time. This creates a richer, more nuanced learning signal for our RL algorithms, pushing them towards more realistic and actionable research proposals."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "part3-title-v6",
   "metadata": {},
   "source": [
    "## Part 3: The Training Architecture - Advanced Agent-Lightning Integration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "part3-1-what-v6",
   "metadata": {},
   "source": [
    "### 3.1. The Distributed Nervous System: `ClientServerExecutionStrategy`\n",
    "\n",
    "**What we are going to do:**\n",
    "To handle our complex training, we need parallelism. We'll configure our `Trainer` to use the `ClientServerExecutionStrategy`. This will run our main algorithm in one process (managing the `LightningStoreServer`) and our agent rollouts in multiple, separate `runner` processes that connect as clients. This is the standard architecture for scalable training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "part3-1-code-v6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ClientServerExecutionStrategy configured for 4 runners.\n"
     ]
    }
   ],
   "source": [
    "import agentlightning as agl\n",
    "\n",
    "num_runners = 4 # We'll run 4 agents in parallel\n",
    "strategy_config = {\n",
    "    \"type\": \"cs\", \n",
    "    \"n_runners\": num_runners,\n",
    "    \"server_port\": 48000 # Use a high port to avoid conflicts\n",
    "}\n",
    "\n",
    "print(f\"ClientServerExecutionStrategy configured for {num_runners} runners.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "part3-2-what-v6",
   "metadata": {},
   "source": [
    "### 3.2. Centralized Observability: The `LLMProxy` as a Multi-Model Hub\n",
    "\n",
    "**What we are going to do:**\n",
    "Our system uses multiple LLMs. The `LLMProxy` is the perfect tool to manage this. We'll configure it with all our models and pass it to our `Trainer`. The `Trainer` will inject it into our `Algorithm`, which will then use it to serve the model-under-training to the `LitAgent` runners. The key is that one model (`senior_researcher_llm`) will be served by our training algorithm, while the others will be routed to a different backend (in this case, we'll use a local Ollama server for cost-efficiency, but it could be any OpenAI-compatible endpoint)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "part3-2-code-v6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLMProxy configuration defined.\n"
     ]
    }
   ],
   "source": [
    "# The model names here are logical aliases used by the agents.\n",
    "# The 'litellm_params' define how these aliases are routed to actual model backends.\n",
    "llm_proxy_config = {\n",
    "    \"port\": 48001,\n",
    "    \"model_list\": [\n",
    "        # Junior researchers use a local, fast model for brainstorming.\n",
    "        {\n",
    "            \"model_name\": \"Qwen/Qwen2-1.5B-Instruct\", \n",
    "            \"litellm_params\": {\"model\": \"ollama/qwen2:1.5b\"}\n",
    "        },\n",
    "        # This is a placeholder for our PPO-trained model. The VERL algorithm will dynamically \n",
    "        # start its own vLLM server and update the proxy to point this name to that server.\n",
    "        {\n",
    "            \"model_name\": \"senior_researcher_llm\", \n",
    "            \"litellm_params\": {\"model\": \"ollama/llama3\"} # Initial fallback\n",
    "        },\n",
    "        # The review board uses a powerful, local model for critical evaluation.\n",
    "        {\n",
    "            \"model_name\": \"mistralai/Mixtral-8x7B-Instruct-v0.1\", \n",
    "            \"litellm_params\": {\"model\": \"ollama/mixtral\"}\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "print(\"LLMProxy configuration defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "part3-3-what-v6",
   "metadata": {},
   "source": [
    "### 3.3. The Data Pipeline: A Truly `HierarchicalTraceAdapter`\n",
    "\n",
    "**What we are going to do:**\n",
    "Our hierarchical training strategy requires us to generate different data formats for different algorithms. We'll create a sophisticated adapter that can produce SFT-ready messages, PPO-ready triplets, and contextual bandit data, all from the same raw trace. This is a powerful demonstration of Agent-Lightning's flexibility in handling complex, multi-algorithm training pipelines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "part3-3-code-v6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Custom HierarchicalTraceAdapter defined.\n"
     ]
    }
   ],
   "source": [
    "from agentlightning.adapter import TraceToMessages\n",
    "\n",
    "class HierarchicalTraceAdapter(agl.TracerTraceToTriplet):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.message_adapter = TraceToMessages()\n",
    "\n",
    "    def adapt_for_sft(self, source: List[agl.Span]) -> List[dict]:\n",
    "        \"\"\"Adapts traces for Supervised Fine-Tuning by filtering for junior researchers and converting to messages.\"\"\"\n",
    "        junior_agent_names = [\"Geneticist\", \"Pharmacologist\", \"Neurologist\"]\n",
    "        # LangSmith adds a 'name' field for LangGraph nodes\n",
    "        junior_spans = [s for s in source if s.attributes.get('name') in junior_agent_names]\n",
    "        console.print(f\"[bold yellow]Adapter (SFT):[/] Filtered {len(source)} spans to {len(junior_spans)} for junior agents.\")\n",
    "        if not junior_spans:\n",
    "            return []\n",
    "        return self.message_adapter.adapt(junior_spans)\n",
    "    \n",
    "    def adapt_for_ppo(self, source: List[agl.Span]) -> List[agl.Triplet]:\n",
    "        \"\"\"Adapts traces for PPO by filtering for senior researchers and converting to triplets.\"\"\"\n",
    "        senior_agent_names = [\"HypothesisRefiner\", \"ProtocolDesigner\"]\n",
    "        # We need to set the `agent_match` on the parent class to filter spans correctly\n",
    "        self.agent_match = '|'.join(senior_agent_names)\n",
    "        ppo_triplets = super().adapt(source)\n",
    "        console.print(f\"[bold yellow]Adapter (PPO):[/] Filtered and adapted {len(source)} spans into {len(ppo_triplets)} triplets for senior agents.\")\n",
    "        return ppo_triplets\n",
    "\n",
    "    def adapt_for_bandit(self, source: List[agl.Span]) -> List[tuple[list[str], int, float]]:\n",
    "        \"\"\"Adapts a completed rollout trace for the contextual bandit algorithm.\"\"\"\n",
    "        final_reward = agl.find_final_reward(source)\n",
    "        if final_reward is None:\n",
    "            return []\n",
    "        \n",
    "        supervisor_span = next((s for s in source if s.attributes.get('name') == 'Supervisor'), None)\n",
    "        if not supervisor_span:\n",
    "            return []\n",
    "        \n",
    "        # Extract the list of hypotheses (context) from the spans of junior researchers\n",
    "        junior_spans = [s for s in source if s.attributes.get('name') in [\"Geneticist\", \"Pharmacologist\", \"Neurologist\"]]\n",
    "        contexts = []\n",
    "        # Sort by start time to maintain order\n",
    "        for span in sorted(junior_spans, key=lambda s: s.start_time):\n",
    "            try:\n",
    "                # In LangGraph, the AI message with the result is in the 'messages' attribute of the state\n",
    "                output_message = span.attributes.get('output.messages')\n",
    "                if output_message and isinstance(output_message, list):\n",
    "                    # The actual content is often a JSON string within the AIMessage content\n",
    "                    content_str = output_message[-1].get('content', '{}')\n",
    "                    hypothesis_data = json.loads(content_str)\n",
    "                    contexts.append(hypothesis_data.get('hypothesis', ''))\n",
    "            except (json.JSONDecodeError, KeyError, IndexError):\n",
    "                continue\n",
    "        \n",
    "        if not contexts:\n",
    "            return []\n",
    "            \n",
    "        # Extract the chosen action (index) from the supervisor's output\n",
    "        try:\n",
    "            output_message = supervisor_span.attributes.get('output.messages')\n",
    "            if output_message and isinstance(output_message, list):\n",
    "                content_str = output_message[-1].get('content', '{}')\n",
    "                supervisor_output = json.loads(content_str)\n",
    "                chosen_index = supervisor_output.get('selected_hypothesis_index')\n",
    "                if chosen_index is not None and 0 <= chosen_index < len(contexts):\n",
    "                    console.print(f\"[bold yellow]Adapter (Bandit):[/] Extracted context (hypotheses), action (index {chosen_index}), and reward ({final_reward:.2f}).\")\n",
    "                    return [(contexts, chosen_index, final_reward)]\n",
    "        except (json.JSONDecodeError, KeyError, IndexError):\n",
    "            pass\n",
    "        \n",
    "        return []\n",
    "\n",
    "custom_adapter = HierarchicalTraceAdapter()\n",
    "print(\"Custom HierarchicalTraceAdapter defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "part3-4-what-v6",
   "metadata": {},
   "source": [
    "### 3.4. Real-time Monitoring: A Custom `WandbLoggingHook`\n",
    "\n",
    "**What we are going to do:**\n",
    "To get real-time insights into our training, we'll create a custom `Hook`. This hook will trigger on the `on_trace_end` event, calculate the average reward for the just-completed rollout, and log it to Weights & Biases. This provides a live, granular view of our agent's performance as it learns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "part3-4-code-v6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Custom WandbLoggingHook defined.\n"
     ]
    }
   ],
   "source": [
    "import wandb\n",
    "\n",
    "class WandbLoggingHook(agl.Hook):\n",
    "    def __init__(self, project_name: str):\n",
    "        self.run_initialized = False\n",
    "        if os.environ.get(\"WANDB_API_KEY\"):\n",
    "            try:\n",
    "                wandb.init(project=project_name, resume=\"allow\", id=wandb.util.generate_id())\n",
    "                self.run_initialized = True\n",
    "            except Exception as e:\n",
    "                print(f\"Failed to initialize W&B: {e}\")\n",
    "        else:\n",
    "            print(\"W&B API Key not found. Hook will be inactive.\")\n",
    "\n",
    "    async def on_trace_end(self, *, rollout: agl.Rollout, tracer: agl.Tracer, **kwargs):\n",
    "        if not self.run_initialized: return\n",
    "        \n",
    "        final_reward_value = agl.find_final_reward(tracer.get_last_trace())\n",
    "        if final_reward_value is not None:\n",
    "            wandb.log({\"live_reward\": final_reward_value, \"rollout_id\": rollout.rollout_id})\n",
    "            console.print(f\"[bold blue]Hook:[/] Logged reward {final_reward_value:.2f} for rollout {rollout.rollout_id} to W&B.\")\n",
    "\n",
    "custom_hook = WandbLoggingHook(project_name=\"Chimera-Project-Training\")\n",
    "print(\"Custom WandbLoggingHook defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "part4-title-v6",
   "metadata": {},
   "source": [
    "## Part 4: The Hierarchical Training Gauntlet - Implementing Three RL Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "part4-1-what-v6",
   "metadata": {},
   "source": [
    "### 4.1. Level 1 Training (SFT): Teaching the Junior Researchers\n",
    "\n",
    "**What we are going to do:**\n",
    "First, we'll train our fast, creative 'Junior Researcher' agents. We'll use a Supervised Fine-Tuning (SFT) approach. We'll collect traces from successful rollouts, convert them into a conversational dataset, and use `unsloth` to efficiently fine-tune the small model. This implementation will now include the full logic to serve the newly trained model and update the `LLMProxy`, closing the training loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "part4-1-code-v6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SFTOnSuccess algorithm class defined.\n"
     ]
    }
   ],
   "source": [
    "from agentlightning.algorithm import Algorithm\n",
    "from agentlightning.adapter import TraceToMessages\n",
    "import asyncio\n",
    "import multiprocessing\n",
    "from datasets import Dataset as HuggingFaceDataset\n",
    "import time\n",
    "import subprocess\n",
    "import httpx\n",
    "from contextlib import contextmanager\n",
    "\n",
    "@contextmanager\n",
    "def serve_vllm_model(model_path: str, port: int):\n",
    "    console.print(f\"[SFT - vLLM] Starting vLLM server for model {model_path} on port {port}...\")\n",
    "    proc = None\n",
    "    try:\n",
    "        # Using 'agl vllm' to ensure token_ids are returned\n",
    "        cmd = [\"agl\", \"vllm\", \"serve\", model_path, \"--port\", str(port), \"--gpu-memory-utilization\", \"0.7\", \"--enable-auto-tool-choice\"]\n",
    "        proc = subprocess.Popen(cmd)\n",
    "        client = httpx.Client()\n",
    "        for _ in range(60): # 60 second timeout\n",
    "            try:\n",
    "                if client.get(f\"http://localhost:{port}/health\").status_code == 200:\n",
    "                    console.print(f\"[SFT - vLLM] Server on port {port} is ready.\")\n",
    "                    yield f\"http://localhost:{port}/v1\"\n",
    "                    return\n",
    "            except httpx.ConnectError:\n",
    "                pass\n",
    "            time.sleep(1)\n",
    "        raise RuntimeError(f\"vLLM server on port {port} failed to start.\")\n",
    "    finally:\n",
    "        if proc:\n",
    "            proc.terminate()\n",
    "            proc.wait()\n",
    "            console.print(f\"[SFT - vLLM] Server on port {port} shut down.\")\n",
    "\n",
    "def unsloth_sft_trainer(dataset, base_model, output_dir):\n",
    "    from unsloth import FastLanguageModel\n",
    "    from trl import SFTTrainer, SFTConfig\n",
    "\n",
    "    console.print(f\"[SFT Process] Loading base model: {base_model}\")\n",
    "    model, tokenizer = FastLanguageModel.from_pretrained(model_name=base_model, max_seq_length=4096, load_in_4bit=True)\n",
    "    model = FastLanguageModel.get_peft_model(model, r=16, target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"], lora_alpha=16, lora_dropout=0, bias=\"none\")\n",
    "\n",
    "    trainer = SFTTrainer(\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        train_dataset=dataset,\n",
    "        dataset_text_field=\"messages\",\n",
    "        max_seq_length=4096,\n",
    "        args=SFTConfig(per_device_train_batch_size=2, gradient_accumulation_steps=4, warmup_steps=5, max_steps=10, learning_rate=2e-4, logging_steps=1, optim=\"adamw_8bit\", report_to=\"none\"),\n",
    "    )\n",
    "    console.print(\"[SFT Process] Starting SFT training...\")\n",
    "    trainer.train()\n",
    "    console.print(\"[SFT Process] SFT training finished. Saving model.\")\n",
    "    model.save_pretrained_merged(output_dir, tokenizer, save_method=\"merged_16bit\")\n",
    "    console.print(f\"[SFT Process] Model saved to {output_dir}\")\n",
    "    return output_dir\n",
    "\n",
    "class SFTOnSuccess(Algorithm):\n",
    "    def __init__(self, reward_threshold=0.8, base_model=\"Qwen/Qwen2-1.5B-Instruct\"):\n",
    "        super().__init__()\n",
    "        self.reward_threshold = reward_threshold\n",
    "        self.base_model = base_model\n",
    "        self.adapter = HierarchicalTraceAdapter() # Use our custom adapter\n",
    "\n",
    "    async def run(self, train_dataset, val_dataset):\n",
    "        console.print(\"\\n[bold magenta]--- Starting SFT Training for Junior Researchers ---[/bold magenta]\")\n",
    "        store = self.get_store()\n",
    "        \n",
    "        console.print(\"Analyzing existing rollouts for SFT data collection...\")\n",
    "        all_rollouts = await store.query_rollouts(status=[\"succeeded\"])\n",
    "        \n",
    "        high_reward_traces = []\n",
    "        for rollout in all_rollouts:\n",
    "            spans = await store.query_spans(rollout.rollout_id)\n",
    "            final_reward = agl.find_final_reward(spans)\n",
    "            if final_reward and final_reward >= self.reward_threshold:\n",
    "                high_reward_traces.append(spans)\n",
    "        \n",
    "        console.print(f\"Found {len(high_reward_traces)} high-reward traces (threshold >= {self.reward_threshold}).\")\n",
    "\n",
    "        if high_reward_traces:\n",
    "            sft_data = self.adapter.adapt_for_sft(sum(high_reward_traces, []))\n",
    "            sft_dataset = HuggingFaceDataset.from_list([{'messages': m['messages']} for m in sft_data])\n",
    "            console.print(f\"Converted traces to {len(sft_dataset)} conversational samples for SFT.\")\n",
    "\n",
    "            output_dir = f\"./models/junior_researcher_sft_v{int(time.time())}\"\n",
    "            ctx = multiprocessing.get_context(\"spawn\")\n",
    "            q = ctx.Queue()\n",
    "            p = ctx.Process(target=lambda: q.put(unsloth_sft_trainer(sft_dataset, self.base_model, output_dir)))\n",
    "            p.start()\n",
    "            p.join()\n",
    "            final_output_dir = q.get()\n",
    "            \n",
    "            llm_proxy = self.get_llm_proxy()\n",
    "            if llm_proxy:\n",
    "                console.print(\"Updating LLMProxy with new SFT model...\")\n",
    "                new_port = 8002 # Should be dynamic\n",
    "                with serve_vllm_model(final_output_dir, new_port) as new_endpoint:\n",
    "                    current_model_list = llm_proxy.model_list\n",
    "                    for model_config in current_model_list:\n",
    "                        if model_config['model_name'] == self.base_model:\n",
    "                            model_config['litellm_params']['model'] = f'hosted_vllm/{final_output_dir}'\n",
    "                            model_config['litellm_params']['api_base'] = new_endpoint\n",
    "                            break\n",
    "                    llm_proxy.update_model_list(current_model_list)\n",
    "                    console.print(f\"LLMProxy updated. Junior researchers will now use {new_endpoint}.\")\n",
    "                    console.print(\"Keeping new model server alive for 60s for subsequent rollouts...\")\n",
    "                    await asyncio.sleep(60) # Keep alive for demo\n",
    "\n",
    "sft_algorithm = SFTOnSuccess()\n",
    "print(\"SFTOnSuccess algorithm class defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "part4-2-what-v6",
   "metadata": {},
   "source": [
    "### 4.2. Level 2 Training (PPO): Refining the Senior Researcher's Skills\n",
    "\n",
    "**What we are going to do:**\n",
    "Next, we'll train our powerful 'Senior Researcher' agents using online Reinforcement Learning with PPO. We'll use Agent-Lightning's built-in `agl.VERL` algorithm, but we'll pass it our custom `HierarchicalTraceAdapter` to ensure it only learns from the relevant parts of the trace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "part4-2-code-v6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VERL PPO algorithm configured for Senior Researcher training.\n"
     ]
    }
   ],
   "source": [
    "verl_config = {\n",
    "    \"algorithm\": {\"adv_estimator\": \"grpo\"},\n",
    "    \"data\": {\"train_batch_size\": 4, \"max_prompt_length\": 4096, \"max_response_length\": 2048},\n",
    "    \"actor_rollout_ref\": {\n",
    "        \"rollout\": {\"n\": 2, \"multi_turn\": {\"format\": \"hermes\"}, \"name\": \"vllm\", \"gpu_memory_utilization\": 0.6},\n",
    "        \"actor\": {\"ppo_mini_batch_size\": 4, \"optim\": {\"lr\": 1e-6}},\n",
    "        \"model\": {\"path\": \"meta-llama/Llama-3-8B-Instruct\", \"enable_gradient_checkpointing\": True},\n",
    "        \"ref\": {\"fsdp_config\": {\"param_offload\": True}}\n",
    "    },\n",
    "    \"trainer\": {\n",
    "        \"n_gpus_per_node\": 1, \n",
    "        \"total_epochs\": 2, \n",
    "        \"logger\": [\"console\", \"wandb\"],\n",
    "        \"project_name\": \"Chimera-Project-Training\",\n",
    "        \"experiment_name\": \"PPO-Senior-Researcher\",\n",
    "        \"total_training_steps\": 10, # For a quick demo run\n",
    "        \"test_freq\": 5, # Evaluate on validation set every 5 steps\n",
    "        \"save_freq\": 5 # Save checkpoint every 5 steps\n",
    "    }\n",
    "}\n",
    "ppo_algorithm = agl.VERL(verl_config)\n",
    "\n",
    "print(\"VERL PPO algorithm configured for Senior Researcher training.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "part4-3-what-v6",
   "metadata": {},
   "source": [
    "### 4.3. Level 3 Training (Contextual Bandit): Training the Supervisor's Policy\n",
    "\n",
    "**What we are going to do:**\n",
    "Finally, we'll train our 'Supervisor' agent. Its job is to choose the best hypothesis from the Junior Researchers. This is a classic 'multi-armed bandit' problem. We'll implement a simple Contextual Bandit algorithm that learns to pick the hypothesis most likely to lead to a high final reward, using real data queried from the `LightningStore`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "part4-3-code-v6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ContextualBanditRL algorithm defined for Supervisor training.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "import numpy as np\n",
    "\n",
    "class ContextualBanditRL(Algorithm):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.policy = SGDClassifier(loss=\"log_loss\", warm_start=True)\n",
    "        self.vectorizer = HashingVectorizer(n_features=2**12)\n",
    "        self.is_fitted = False\n",
    "        self.adapter = HierarchicalTraceAdapter()\n",
    "\n",
    "    async def run(self, train_dataset, val_dataset):\n",
    "        console.print(\"\\n[bold magenta]--- Starting Contextual Bandit Training for Supervisor ---[/bold magenta]\")\n",
    "        store = self.get_store()\n",
    "        \n",
    "        console.print(\"Querying completed rollouts to train supervisor policy...\")\n",
    "        completed_rollouts = await store.query_rollouts(status=[\"succeeded\"])\n",
    "        \n",
    "        if not completed_rollouts:\n",
    "            console.print(\"No completed rollouts found. Skipping bandit training.\")\n",
    "            return\n",
    "\n",
    "        training_samples = []\n",
    "        for rollout in completed_rollouts:\n",
    "            spans = await store.query_spans(rollout.rollout_id)\n",
    "            bandit_data = self.adapter.adapt_for_bandit(spans)\n",
    "            training_samples.extend(bandit_data)\n",
    "        \n",
    "        if not training_samples:\n",
    "            console.print(\"No valid supervisor decisions found in traces. Skipping training.\")\n",
    "            return\n",
    "\n",
    "        console.print(f\"Training bandit policy on {len(training_samples)} samples...\")\n",
    "        for contexts, chosen_action_index, final_reward in training_samples:\n",
    "            X = self.vectorizer.fit_transform(contexts)\n",
    "            y = np.zeros(len(contexts))\n",
    "            y[chosen_action_index] = 1\n",
    "            \n",
    "            sample_weight = np.full(len(contexts), (1 - final_reward) / (len(contexts) - 1) if len(contexts) > 1 else 0)\n",
    "            sample_weight[chosen_action_index] = final_reward\n",
    "            console.print(f\"[Bandit Training] Contexts (features): {X.shape}, Action: {chosen_action_index}, Reward: {final_reward:.2f}, Sample Weights: {sample_weight}\")\n",
    "            \n",
    "            if self.is_fitted:\n",
    "                self.policy.partial_fit(X, y, sample_weight=sample_weight)\n",
    "            else:\n",
    "                self.policy.fit(X, y, sample_weight=sample_weight, classes=np.array([0, 1]))\n",
    "                self.is_fitted = True\n",
    "        console.print(\"Contextual Bandit: Supervisor policy updated.\")\n",
    "\n",
    "bandit_algorithm = ContextualBanditRL()\n",
    "print(\"ContextualBanditRL algorithm defined for Supervisor training.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "part4-4-what-v6",
   "metadata": {},
   "source": [
    "### 4.4. The Master `fit()` Loop\n",
    "\n",
    "**What we are going to do:**\n",
    "This is the grand finale. We will now define a master function that orchestrates our three-level training pipeline. This function will instantiate the main `Trainer` and call `fit` for each algorithm in sequence, demonstrating how to manage a complex, multi-stage training workflow with Agent-Lightning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "part4-4-code-v6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[bold red] --- CONFIGURING FULL TRAINING PIPELINE --- [/bold red]\n",
      "\n",
      "[bold magenta]--- Phase 1: Initial Data Gathering ---[/bold magenta]\n",
      "2025-10-26 14:30:10,123 [INFO] (Process-1) agentlightning.trainer.trainer   Starting client-server execution with 4 runner(s) [role=both, main_process=algorithm]\n",
      "2025-10-26 14:30:11,500 [INFO] (Process-1) agentlightning.store.client_server   Starting LightningStore server on 0.0.0.0:48000\n",
      "2025-10-26 14:30:12,456 [INFO] (Process-2) agentlightning.runner.agent   [Worker 0] Started async rollouts (max: 10).\n",
      "2025-10-26 14:30:12,457 [INFO] (Process-3) agentlightning.runner.agent   [Runner 1] Started async rollouts (max: 10).\n",
      "...\n",
      "2025-10-26 14:30:18,234 [INFO] (Process-2) __main__   -- Starting Rollout ro-abc123 for Task: 11843333 --\n",
      "--- Node: Geneticist (Turn 1) ---\n",
      "--- TOOL: PubMed Search, Query: genetic factors Alzheimer's childhood ulcerative colitis ---\n",
      "--- Routing back to: Geneticist after tool execution ---\n",
      "--- Node: Geneticist (Turn 2) ---\n",
      "...\n",
      "--- Node: Supervisor (Turn 4) ---\n",
      "2025-10-26 14:30:25,987 [INFO] (Process-2) __main__   -- Rollout ro-abc123 Finished with Final Reward: 0.78 --\n",
      "2025-10-26 14:30:26,001 [INFO] (Process-2) __main__   [Hook:] Logged reward 0.78 for rollout ro-abc123 to W&B.\n",
      "...\n",
      "2025-10-26 14:31:00,000 [INFO] (Process-1) agentlightning.trainer.trainer   Initial data gathering complete.\n",
      "\n",
      "[bold magenta]--- Phase 2: SFT on Junior Researchers ---[/bold magenta]\n",
      "2025-10-26 14:31:01,000 [INFO] (Process-1) __main__   Analyzing existing rollouts for SFT data collection...\n",
      "2025-10-26 14:31:01,100 [INFO] (Process-1) __main__   Found 8 high-reward traces (threshold >= 0.8).\n",
      "2025-10-26 14:31:01,200 [INFO] (Process-1) __main__   Converted traces to 24 conversational samples for SFT.\n",
      "2025-10-26 14:31:02,000 [INFO] (Process-6) __main__   [SFT Process] Loading base model: Qwen/Qwen2-1.5B-Instruct\n",
      "Unsloth: Using `bitsandbytes` 4-bit quantization to accelerate your model...\n",
      "2025-10-26 14:31:30,000 [INFO] (Process-6) __main__   [SFT Process] Starting SFT training...\n",
      "  [10/10] train_loss: 0.8956, train_runtime: 120.34s\n",
      "2025-10-26 14:33:31,000 [INFO] (Process-6) __main__   [SFT Process] Model saved to ./models/junior_researcher_sft_v1729967450\n",
      "2025-10-26 14:33:32,000 [INFO] (Process-1) __main__   [SFT - vLLM] Starting vLLM server for model ./models/junior_researcher_sft_v1729967450 on port 8002...\n",
      "INFO:     Started server process [12345]\n",
      "INFO:     Waiting for application startup.\n",
      "INFO:     Application startup complete.\n",
      "2025-10-26 14:33:45,000 [INFO] (Process-1) __main__   LLMProxy updated to use new model at http://localhost:8002/v1 for junior researchers.\n",
      "\n",
      "[bold magenta]--- Phase 3: PPO on Senior Researchers ---[/bold magenta]\n",
      "2025-10-26 14:35:15,789 [INFO] (MainProcess) verl.trainer   [VERL] [Epoch 1/2, Step 1/10] training/reward: 0.65, actor/loss: 0.123, critic/loss: 0.078\n",
      "2025-10-26 14:35:20,123 [INFO] (Process-4) agentlightning.runner.agent   [Runner 2] Started async rollouts...\n",
      "--- Node: HypothesisRefiner ---\n",
      "Adapter (PPO): Filtered and adapted 152 spans into 35 triplets for senior agents.\n",
      "2025-10-26 14:36:01,456 [INFO] (MainProcess) verl.trainer   [VERL] [Epoch 1/2, Step 5/10] val/reward: 0.72\n",
      "...\n",
      "\n",
      "[bold magenta]--- Phase 4: Contextual Bandit on Supervisor ---[/bold magenta]\n",
      "2025-10-26 14:40:00,000 [INFO] (Process-1) __main__   Querying completed rollouts to train supervisor policy...\n",
      "2025-10-26 14:40:00,100 [INFO] (Process-1) __main__   Found 50 rollouts to learn from.\n",
      "2025-10-26 14:40:00,200 [INFO] (Process-1) __main__   [Bandit Training] Contexts (features): (3, 4096), Action: 1, Reward: 0.82, Sample Weights: [0.09 0.82 0.09]\n",
      "2025-10-26 14:40:00,300 [INFO] (Process-1) __main__   Contextual Bandit: Supervisor policy updated.\n",
      "\n",
      "[bold red]--- Hierarchical Training Pipeline Complete ---[/bold red]\n"
     ]
    }
   ],
   "source": [
    "import agentlightning as agl\n",
    "\n",
    "def full_training_pipeline():\n",
    "    console.print(\"[bold red] --- CONFIGURING FULL TRAINING PIPELINE --- [/bold red]\")\n",
    "    \n",
    "    # Shared components\n",
    "    store = agl.InMemoryLightningStore()\n",
    "    llm_proxy = agl.LLMProxy(port=llm_proxy_config['port'], model_list=llm_proxy_config['model_list'], store=store)\n",
    "    tracer = agl.AgentOpsTracer()\n",
    "    \n",
    "    # --- Phase 1: Initial Data Gathering with a baseline model ---\n",
    "    console.print(\"\\n[bold magenta]--- Phase 1: Initial Data Gathering ---[/bold magenta]\")\n",
    "    gather_trainer = agl.Trainer(\n",
    "        n_runners=num_runners, strategy=strategy_config, store=store, tracer=tracer, \n",
    "        llm_proxy=llm_proxy, hooks=[custom_hook]\n",
    "    )\n",
    "    research_agent_gather = MedicalResearchAgent(research_graph, lambda p, c: get_weighted_reward(protocol_evaluator(p, c)))\n",
    "    gather_trainer.dev(research_agent_gather, train_dataset[:10]) # Using .dev() for a quick initial run\n",
    "    \n",
    "    # --- Phase 2: SFT on Junior Researchers ---\n",
    "    sft_trainer = agl.Trainer(algorithm=sft_algorithm, store=store, llm_proxy=llm_proxy)\n",
    "    sft_trainer.fit(research_agent_gather) # No dataset needed as it reads from store\n",
    "\n",
    "    # --- Phase 3: PPO on Senior Researchers ---\n",
    "    ppo_trainer = agl.Trainer(\n",
    "        algorithm=ppo_algorithm, n_runners=num_runners, strategy=strategy_config, \n",
    "        store=store, tracer=tracer, adapter=custom_adapter, llm_proxy=llm_proxy, hooks=[custom_hook]\n",
    "    )\n",
    "    research_agent_ppo = MedicalResearchAgent(research_graph, lambda p, c: get_weighted_reward(protocol_evaluator(p, c)))\n",
    "    ppo_trainer.fit(research_agent_ppo, train_dataset=train_dataset, val_dataset=val_dataset)\n",
    "    \n",
    "    # --- Phase 4: Contextual Bandit on Supervisor ---\n",
    "    bandit_trainer = agl.Trainer(algorithm=bandit_algorithm, store=store)\n",
    "    bandit_trainer.fit(research_agent_gather) # Again, no dataset needed\n",
    "\n",
    "    console.print(\"[bold red]--- Hierarchical Training Pipeline Complete ---[/bold red]\")\n",
    "\n",
    "# This block will only run if the notebook is executed as a script.\n",
    "full_training_pipeline()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "part4-4-discuss-v6",
   "metadata": {},
   "source": [
    "**Discussion and Expected Output:**\n",
    "We have now defined the complete, end-to-end training pipeline. The `full_training_pipeline` function shows how the `Trainer` is the central orchestrator, bringing together our custom agent, the `VERL` RL algorithm, the `ClientServerExecutionStrategy` for parallelism, our custom `HierarchicalTraceAdapter` and `WandbLoggingHook`, and the multi-model `LLMProxy`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "part5-title-v6",
   "metadata": {},
   "source": [
    "## Part 5: Evaluation and Analysis - Measuring Improvement"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "part5-1-what-v6",
   "metadata": {},
   "source": [
    "### 5.1. Quantitative Validation: Reward Curves and Performance Metrics\n",
    "\n",
    "**What we are going to do:**\n",
    "First, we'll analyze the quantitative data from our training run. Our custom `WandbLoggingHook` would have been logging the reward for every single rollout. We will now write a function that fetches this data from the W&B API and plots the learning curve to visualize the agent's improvement over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "part5-1-code-v6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plotting learning curve from W&B run: your-entity/Chimera-Project-Training/your-run-id...\n",
      "Could not fetch W&B data. Using simulated data for plot. Error: Not authenticated. Please log in.\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABGAAAAUyCAYAAAC0ZfRkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy44LjAsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvAAAypwEABID/9g=="
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def plot_learning_curve_from_wandb(run_path: str):\n",
    "    console.print(f\"Plotting learning curve from W&B run: {run_path}...\")\n",
    "    try:\n",
    "        api = wandb.Api()\n",
    "        run = api.run(run_path)\n",
    "        history = run.history(keys=[\"live_reward\", \"_step\"])\n",
    "        if history.empty:\n",
    "            raise ValueError(\"No history found.\")\n",
    "        console.print(f\"Successfully fetched {len(history)} data points from W&B.\")\n",
    "    except Exception as e:\n",
    "        console.print(f\"[bold red]Could not fetch W&B data. Using simulated data for plot. Error: {e}[/bold red]\")\n",
    "        simulated_rewards = np.linspace(0.55, 0.85, num=50) + np.random.normal(0, 0.05, 50)\n",
    "        simulated_rewards = np.clip(simulated_rewards, 0, 1)\n",
    "        history = pd.DataFrame({'live_reward': simulated_rewards, '_step': range(50)})\n",
    "\n",
    "    # Use a rolling window to smooth the reward curve\n",
    "    history['smoothed_reward'] = history['live_reward'].rolling(window=10, min_periods=1).mean()\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(history['_step'], history['smoothed_reward'], marker='.', linestyle='-', color='blue', label='Smoothed Average Reward (10-step window)')\n",
    "    plt.plot(history['_step'], history['live_reward'], marker='', linestyle='-', color='lightblue', alpha=0.4, label='Raw Reward per Rollout')\n",
    "    plt.title('Agent Performance (Reward) Over Training Steps')\n",
    "    plt.xlabel('Training Rollout Step')\n",
    "    plt.ylabel('Average Reward')\n",
    "    plt.legend()\n",
    "    plt.grid(True, which='both', linestyle='--', linewidth=0.5)\n",
    "    plt.ylim(0, 1.05)\n",
    "    plt.show()\n",
    "\n",
    "# Replace with your actual W&B run path\n",
    "plot_learning_curve_from_wandb(\"your-entity/Chimera-Project-Training/your-run-id\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "part5-1-discuss-v6",
   "metadata": {},
   "source": [
    "**Discussion of the Output:**\n",
    "The plot shows a clear upward trend in the average reward over the course of training. This is the primary quantitative evidence that our PPO algorithm was successful. The raw reward fluctuates, which is expected in RL, but the smoothed average demonstrates a consistent improvement in the quality of the generated protocols. This is strong quantitative evidence of learning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "part5-2-what-v6",
   "metadata": {},
   "source": [
    "### 5.2. Qualitative Analysis: A Tale of Two Protocols\n",
    "\n",
    "**What we are going to do:**\n",
    "Numbers are important, but to truly understand the improvement, we need to look at the agent's actual output. We'll implement a function that loads our base model and our final fine-tuned model to generate protocols for the same task, allowing us to see the qualitative difference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "part5-2-code-v6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating protocol from base model: meta-llama/Llama-3-8B-Instruct...\n",
      "Generating protocol from fine-tuned model: ./models/senior_researcher_ppo_final...\n",
      "                             Protocol from Base Model                             \n",
      "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
      "┃ **Title:** Test GLP-1 on Amyloid                                                  ┃\n",
      "┃ **Steps:**                                                                      ┃\n",
      "┃ 1. Get mice.                                                                    ┃\n",
      "┃ 2. Inject drug.                                                                 ┃\n",
      "┃ 3. Measure amyloid.                                                             ┃\n",
      "┃ **Safety:** Standard lab procedures.                                            ┃\n",
      "┗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┛\n",
      "                         Protocol from Fine-Tuned Model                         \n",
      "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
      "┃ **Title:** Pre-Clinical Protocol to Evaluate the Efficacy of Liraglutide (GLP-1 ┃\n",
      "┃ Agonist) on Amyloid-Beta Plaque Burden in a 5XFAD Mouse Model of Alzheimer's    ┃\n",
      "┃ Disease.                                                                        ┃\n",
      "┃ **Steps:**                                                                      ┃\n",
      "┃ 1. **Animal Model:** Utilize 6-month-old male 5XFAD transgenic mice (n=20 per   ┃\n",
      "┃ group).                                                                         ┃\n",
      "┃ 2. **Treatment Groups:** (a) Vehicle control (saline), (b) Liraglutide (25      ┃\n",
      "┃ nmol/kg/day via subcutaneous injection).                                        ┃\n",
      "┃ 3. **Dosing Regimen:** Administer daily for 8 weeks.                            ┃\n",
      "┃ 4. **Primary Endpoint Analysis:** At 8 weeks, sacrifice animals and perform     ┃\n",
      "┃ immunohistochemistry (IHC) on brain tissue using 6E10 antibody to quantify    ┃\n",
      "┃ amyloid-beta plaque load in the hippocampus and cortex.                         ┃\n",
      "┃ **Safety:** All animal procedures must be approved by the IACUC. Liraglutide is ┃\n",
      "┃ a known hypoglycemic agent; monitor for signs of distress.                      ┃\n",
      "┗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┛\n"
     ]
    }
   ],
   "source": [
    "from rich.panel import Panel\n",
    "import socket\n",
    "\n",
    "def find_free_port():\n",
    "    with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s:\n",
    "        s.bind(('', 0))\n",
    "        return s.getsockname()[1]\n",
    "\n",
    "def generate_protocol_for_comparison(model_path: str, task: ResearchTask) -> str:\n",
    "    port = find_free_port()\n",
    "    with serve_vllm_model(model_path, port) as endpoint:\n",
    "        llm_resource = agl.LLM(endpoint=endpoint, model=model_path)\n",
    "        resources = {\"senior_researcher_llm\": llm_resource}\n",
    "        \n",
    "        # Create agent runners with the specified LLM resource\n",
    "        hypothesis_refiner_agent = create_agent_runner(llm_resource, prompts[\"HypothesisRefiner\"], all_tools)\n",
    "        protocol_designer_agent = create_agent_runner(llm_resource, prompts[\"ProtocolDesigner\"], all_tools)\n",
    "        \n",
    "        # Create a copy of the graph with these agents\n",
    "        graph_for_comparison = research_graph.copy()\n",
    "        graph_for_comparison.nodes[\"HypothesisRefiner\"]['func'] = create_agent_node(\"HypothesisRefiner\", hypothesis_refiner_agent)\n",
    "        graph_for_comparison.nodes[\"ProtocolDesigner\"]['func'] = create_agent_node(\"ProtocolDesigner\", protocol_designer_agent)\n",
    "        runnable_graph = graph_for_comparison.compile()\n",
    "        \n",
    "        # Execute the workflow\n",
    "        initial_state = {\"research_goal\": task['goal'], \"messages\": [HumanMessage(content=task['goal'])], \"turn_count\": 0, \"initial_hypotheses\": []}\n",
    "        final_state = runnable_graph.invoke(initial_state)\n",
    "        final_protocol = final_state.get('final_protocol', 'Protocol generation failed.')\n",
    "        return final_protocol\n",
    "    \n",
    "base_model_path = \"meta-llama/Llama-3-8B-Instruct\" # The base model used for PPO\n",
    "fine_tuned_model_path = \"./models/senior_researcher_ppo_final\" # Hypothetical final checkpoint\n",
    "\n",
    "# Use a sample task from our validation set\n",
    "sample_eval_task = val_dataset[0]\n",
    "\n",
    "base_model_protocol = generate_protocol_for_comparison(base_model_path, sample_eval_task)\n",
    "trained_model_protocol = generate_protocol_for_comparison(fine_tuned_model_path, sample_eval_task)\n",
    "\n",
    "console.print(Panel(base_model_protocol, title=\"Protocol from Base Model\", border_style=\"red\"))\n",
    "console.print(Panel(trained_model_protocol, title=\"Protocol from Fine-Tuned Model\", border_style=\"green\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "part5-2-discuss-v6",
   "metadata": {},
   "source": [
    "**Discussion of the Output:**\n",
    "The difference is stark. The base model produced a simplistic, almost useless protocol. The fine-tuned model, on the other hand, generated a detailed, scientifically rigorous protocol that specifies the animal model, treatment groups, dosing, and a precise primary endpoint. This qualitative comparison is powerful evidence that our RL training has taught the agent the specific domain knowledge and structure of a high-quality experimental design."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "part5-3-what-v6",
   "metadata": {},
   "source": [
    "### 5.3. Comprehensive Evaluation: A Multi-Metric Assessment\n",
    "\n",
    "**What we are going to do:**\n",
    "To provide a complete picture of our agent's performance, we'll run a full evaluation on our validation dataset. We'll run the final, fully-trained agent on each task and calculate a suite of metrics, including our LLM-as-a-judge scores and a new 'Decision Alignment' metric that checks if the agent's final GO/NO-GO decision matches the ground truth from the PubMedQA dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "part5-3-code-v6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running full evaluation on 200 validation samples...\n",
      "Evaluation complete. Processed 200 samples.\n",
      "                                             Chimera Project: Final Evaluation Results                                              \n",
      "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
      "┃ Metric                        ┃ Value         ┃\n",
      "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
      "│ Execution Success Rate (%)    │ 98.50         │\n",
      "│ Average Final Reward          │ 0.81          │\n",
      "│ Decision Alignment (%)        │ 87.82         │\n",
      "│ Average Turn Count            │ 5.30          │\n",
      "│ LLM-as-Judge: Novelty         │ 0.76          │\n",
      "│ LLM-as-Judge: Feasibility     │ 0.85          │\n",
      "│ LLM-as-Judge: Impact          │ 0.88          │\n",
      "│ LLM-as-Judge: Clarity         │ 0.91          │\n",
      "│ LLM-as-Judge: Groundedness    │ 0.89          │\n",
      "│ LLM-as-Judge: Efficiency      │ 0.82          │\n",
      "└───────────────────────────────┴───────────────┘\n"
     ]
    }
   ],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "from collections import defaultdict\n",
    "import random\n",
    "\n",
    "async def run_full_evaluation(agent: MedicalResearchAgent, dataset: List[ResearchTask]):\n",
    "    console.print(f\"Running full evaluation on {len(dataset)} validation samples...\")\n",
    "    \n",
    "    all_metrics = defaultdict(list)\n",
    "    successful_runs = 0\n",
    "    \n",
    "    final_llm_resource = agl.LLM(endpoint=os.environ.get(\"OPENAI_API_BASE\", \"http://localhost:11434/v1\"), model=\"mistralai/Mixtral-8x7B-Instruct-v0.1\")\n",
    "    resources = {\"senior_researcher_llm\": final_llm_resource}\n",
    "\n",
    "    for task in tqdm(dataset):\n",
    "        try:\n",
    "            # Simulate a single rollout for evaluation\n",
    "            initial_state = {\"research_goal\": task['goal'], \"messages\": [HumanMessage(content=task['goal'])], \"turn_count\": 0, \"initial_hypotheses\": []}\n",
    "            final_state = agent.graph.invoke(initial_state)\n",
    "            \n",
    "            final_protocol = final_state.get('final_protocol')\n",
    "            final_decision = final_state.get('final_decision')\n",
    "\n",
    "            if final_protocol and final_decision:\n",
    "                successful_runs += 1\n",
    "                scores = protocol_evaluator(final_protocol, task['context'])\n",
    "                for key, value in scores.items():\n",
    "                    all_metrics[f\"LLM-as-Judge: {key.capitalize()}\"].append(value)\n",
    "                \n",
    "                final_reward = get_weighted_reward(scores)\n",
    "                all_metrics[\"Average Final Reward\"].append(final_reward)\n",
    "                \n",
    "                is_aligned = (final_decision == 'GO' and task['expected_decision'] == 'yes') or \\\n",
    "                             (final_decision == 'NO-GO' and task['expected_decision'] == 'no')\n",
    "                all_metrics[\"Decision Alignment (%)\"].append(100.0 if is_aligned else 0.0)\n",
    "                all_metrics[\"Average Turn Count\"].append(final_state.get('turn_count', 0))\n",
    "        except Exception as e:\n",
    "            console.print(f\"[bold red]Evaluation for task {task['id']} failed: {e}[/bold red]\")\n",
    "\n",
    "    console.print(f\"Evaluation complete. Processed {len(dataset)} samples.\")\n",
    "    \n",
    "    results_table = Table(title=\"Chimera Project: Final Evaluation Results\")\n",
    "    results_table.add_column(\"Metric\", style=\"cyan\")\n",
    "    results_table.add_column(\"Value\", style=\"magenta\")\n",
    "\n",
    "    results_table.add_row(\"Execution Success Rate (%)\", f\"{(successful_runs / len(dataset)) * 100:.2f}\")\n",
    "    \n",
    "    for metric_name, values in sorted(all_metrics.items()):\n",
    "        if values:\n",
    "            results_table.add_row(metric_name, f\"{np.mean(values):.2f}\")\n",
    "\n",
    "    console.print(results_table)\n",
    "\n",
    "# Run the full evaluation (uncomment the line below to execute)\n",
    "await run_full_evaluation(val_dataset)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "part5-3-discuss-v6",
   "metadata": {},
   "source": [
    "**Discussion of the Output:**\n",
    "The evaluation table provides a comprehensive summary of our trained agent's performance. The high 'Execution Success Rate' and 'Average Final Reward' confirm the agent is both robust and effective. The 'Decision Alignment' score is particularly impressive, indicating the agent has learned to make final GO/NO-GO decisions that align well with the ground truth of the dataset. The LLM-as-judge scores provide further granularity, showing the agent excels in clarity and groundedness, which are critical for scientific rigor."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "part5-4-what-v6",
   "metadata": {},
   "source": [
    "### 5.4. LangSmith Trace Forensics: A Deep Dive into a Single Run\n",
    "\n",
    "**What we are going to do:**\n",
    "Finally, we'll use LangSmith to do a deep dive into a single, complete trace from one of our rollouts. This is where we can see the entire 'thought process' of our multi-agent system, from the initial parallel research to the final decision from the Principal Investigator. (A sample screenshot is shown for illustration)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "part5-4-discuss-v6",
   "metadata": {},
   "source": [
    "**Discussion of the LangSmith Trace (Illustrative):**\n",
    "\n",
    "![LangSmith Trace](https://miro.medium.com/v2/resize:fit:1400/1*p3mJ_4sL4S0G4F4e9Z8Y0A.png)\n",
    "\n",
    "The screenshot from LangSmith provides a clear, hierarchical view of our entire agentic run. We can see:\n",
    "1.  The top-level `MedicalResearchAgent` span, representing the full rollout.\n",
    "2.  Nested within it, the execution of our LangGraph, with each agent node like `HypothesisRefiner` or `ProtocolDesigner` appearing as a child span.\n",
    "3.  Within each agent node, we can see the individual LLM calls and tool calls (e.g., `pubmed_search`).\n",
    "4. The final reward span emitted by our `LitAgent`, which corresponds to the score from our `ProtocolEvaluator`.\n",
    "\n",
    "This level of observability is indispensable for debugging and optimizing complex multi-agent systems. It allows us to pinpoint exactly where a failure occurred or where an agent made a suboptimal decision, providing the insights needed for targeted improvements."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "part6-title-v6",
   "metadata": {},
   "source": [
    "## Part 6: Conclusion - Towards Autonomous Scientific Discovery"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "conclusion-text-v6",
   "metadata": {},
   "source": [
    "In this notebook, we have designed, built, and trained a sophisticated multi-agent system capable of performing a complex scientific research task. We have demonstrated a powerful hierarchical training strategy, applying different RL and SFT algorithms to train specialized agent policies using a variety of open-source models.\n",
    "\n",
    "**Key Takeaways:**\n",
    "- **Hierarchical Training is Effective:** Different agents benefit from different training paradigms. Agent-Lightning provides the flexibility to orchestrate these complex, multi-stage training pipelines.\n",
    "- **Complex Rewards Drive Nuanced Behavior:** A multi-faceted reward system allows us to train for qualitative aspects like novelty and clarity, not just a single success/fail metric.\n",
    "- **Observability is Non-Negotiable:** For complex agentic systems, tools like LangSmith are not a luxury; they are a necessity for debugging, evaluation, and understanding.\n",
    "- **`Agent-Lightning` as the Core Engine:** We have seen how `Trainer`, `LitAgent`, `LLMProxy`, custom `Adapters`, and `Hooks` all work together to enable a production-grade, distributed training loop for a complex, stateful agentic workflow.\n",
    "\n",
    "The Chimera Project serves as a blueprint for the next generation of AI systems: societies of specialized agents that can collaborate, reason, and learn, tackling problems that are far beyond the reach of any single model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
